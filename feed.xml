<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.4.1">Jekyll</generator><link href="https://hlinhn.github.io/feed.xml" rel="self" type="application/atom+xml" /><link href="https://hlinhn.github.io/" rel="alternate" type="text/html" hreflang="en" /><updated>2025-07-29T05:28:31+00:00</updated><id>https://hlinhn.github.io/feed.xml</id><title type="html">blank</title><subtitle>Things I learn along the way
</subtitle><entry><title type="html">Contact models</title><link href="https://hlinhn.github.io/blog/2025/contact/" rel="alternate" type="text/html" title="Contact models" /><published>2025-07-29T00:00:00+00:00</published><updated>2025-07-29T00:00:00+00:00</updated><id>https://hlinhn.github.io/blog/2025/contact</id><content type="html" xml:base="https://hlinhn.github.io/blog/2025/contact/"><![CDATA[<h3 id="introduction">Introduction</h3>
<p>In human motion generation and recovery, contact events are frequently used to judge whether interactions with the environment are plausible. This is because meaningful motions are only possible through interactions with the world: from simple everyday motions like walking or picking up an object to more difficult actions such as parkour or gymnastics.</p>

<p>Detecting contact events are usually done via proximity if 3D models are known. Alternatively a model can also be trained to detect contact between body and environment in 2D images or videos. Measuring the magnitude of the resulting forces is a different matter altogether, however. In this post we discuss some common methods of doing so.</p>

<p>Why do we care about the magnitude of the contact forces, especially in the human motion related tasks? While kinematics observations alone are very successful in recreating plausible-looking motions, the lack of dynamics modeling means that rarer motions are difficult to replicate, and generalizations to unseen scenes and interactions suffer. Success in creating plausible trajectories does not imply success in inferring underlying dynamics, as shown in <sup><a href="#1">[1]</a></sup></p>

<p><a id="1"><sub>[1]</sub></a>
<sub>Vafa, K., Chang, P.G., Rambachan, A., &amp; Mullainathan, S. (2025). What Has a Foundation Model Found? Using Inductive Bias to Probe for World Models.</sub></p>]]></content><author><name></name></author><category term="physics" /><summary type="html"><![CDATA[Introduction In human motion generation and recovery, contact events are frequently used to judge whether interactions with the environment are plausible. This is because meaningful motions are only possible through interactions with the world: from simple everyday motions like walking or picking up an object to more difficult actions such as parkour or gymnastics.]]></summary></entry><entry><title type="html">Physics in human motion</title><link href="https://hlinhn.github.io/blog/2024/physics/" rel="alternate" type="text/html" title="Physics in human motion" /><published>2024-02-19T00:00:00+00:00</published><updated>2024-02-19T00:00:00+00:00</updated><id>https://hlinhn.github.io/blog/2024/physics</id><content type="html" xml:base="https://hlinhn.github.io/blog/2024/physics/"><![CDATA[<p>There are 3 levels of physics knowledge integration into deep learning methods<sup><a href="#1">[1]</a></sup>:</p>
<ul>
  <li>use the data generated from physics model. This is the least involved level, where the model equations are only used as a source to generate data for supervised training.</li>
  <li>use soft constraints in the form of physics based loss term. This is where approaches like PINN belong.</li>
  <li>use differentiable physics simulator. This is the tightest coupling currently, where the network weights are updated with the gradients calculated from an integrated simulator.</li>
</ul>

<p>In human motion related problems, supervised training with real data dominates. Simulated data are not as popular because of the complexity and multimodality of human movement. However, we still can find some works that are close to this approach, such as PhysDiff<sup><a href="#5">[5]</a></sup>, which provides physics denoised data for the diffusion process by pushing the intermediate results through a physics simulator.</p>

<p>The second approach is not usually utilized the same way as in PINN, because the PDE describing human motion includes discrete contact terms that are difficult to get gradients from. 
However, intuitive physics loss terms such as simple stability and ground constraints are getting more attention, for example IPMAN<sup><a href="#2">[2]</a></sup>. Another slightly related example is InterDiff<sup><a href="#4">[4]</a></sup>, where the trajectories of the interacting objects are kept simple by transformation to appropriate frames.</p>

<p>One work that uses the final approach is <strong>Weakly supervised learning of human dynamics</strong><sup><a href="#3">[3]</a></sup>. The paper did not incorporate a physics simulator, instead it used simple forward dynamics calculations supplied with prediction of contact information (which is also the approach of DnD<sup><a href="#6">[6]</a></sup>). It would be interesting to check if SIP training would improve its performance. Probably likewise for DnD, however the training time could be prohibitive. Another paper much closer to the spirit of the final approach is DiffPhy<sup><a href="#7">[7]</a></sup>, where TDS is used in the reconstruction loop.</p>

<p><a id="1"><sub>[1]</sub></a>
<sub>Thuerey, Nils et al. “Physics-based Deep Learning.” ArXiv abs/2109.05237 (2021): n. pag.</sub></p>

<p><a id="2"><sub>[2]</sub></a>
<sub>Tripathi, Shashank et al. “3D Human Pose Estimation via Intuitive Physics.” 2023 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) (2023): 4713-4725.</sub></p>

<p><a id="3"><sub>[3]</sub></a><sub> Zell, Petrissa et al. “Weakly-supervised Learning of Human Dynamics.” European Conference on Computer Vision (2020).</sub></p>

<p><a id="4"><sub>[4]</sub></a><sub> Xu, Sirui et al. “InterDiff: Generating 3D Human-Object Interactions with Physics-Informed Diffusion.” 2023 IEEE/CVF International Conference on Computer Vision (ICCV) (2023): 14882-14894.</sub></p>

<p><a id="5"><sub>[5]</sub></a><sub> Yuan, Ye et al. “PhysDiff: Physics-Guided Human Motion Diffusion Model.” 2023 IEEE/CVF International Conference on Computer Vision (ICCV) (2022): 15964-15975.</sub></p>

<p><a id="6"><sub>[6]</sub></a><sub> Li, Jiefeng et al. “D&amp;D: Learning Human Dynamics from Dynamic Camera.” European Conference on Computer Vision (2022).</sub></p>

<p><a id="7"><sub>[7]</sub></a><sub> Gartner, Erik et al. “Differentiable Dynamics for Articulated 3d Human Motion Reconstruction.” 2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) (2022): 13180-13190.</sub></p>]]></content><author><name></name></author><category term="physics" /><summary type="html"><![CDATA[There are 3 levels of physics knowledge integration into deep learning methods[1]: use the data generated from physics model. This is the least involved level, where the model equations are only used as a source to generate data for supervised training. use soft constraints in the form of physics based loss term. This is where approaches like PINN belong. use differentiable physics simulator. This is the tightest coupling currently, where the network weights are updated with the gradients calculated from an integrated simulator.]]></summary></entry><entry><title type="html">HumanML3D actions</title><link href="https://hlinhn.github.io/blog/2023/humanml3d/" rel="alternate" type="text/html" title="HumanML3D actions" /><published>2023-08-29T00:00:00+00:00</published><updated>2023-08-29T00:00:00+00:00</updated><id>https://hlinhn.github.io/blog/2023/humanml3d</id><content type="html" xml:base="https://hlinhn.github.io/blog/2023/humanml3d/"><![CDATA[<p>I was curious about the actions in HumanML3D, so I made a simple word cloud out of the descriptions. After removing a few nuisance words (in, at, out, up, to, etc - there were a lot of person and man as well)</p>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/2023-08-29/wc-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/2023-08-29/wc-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/2023-08-29/wc-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/2023-08-29/wc.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture>

</figure>

    </div>
</div>
<div class="caption">
    Simple HumanML3D word cloud from 29232 files, including the mirrored actions. We removed a lot of positional and directional words to make the actions stand out more. It seems we have a lot of walking action here.
</div>

<p>Then I realized I didn’t need to do this - I should be able to get all the verbs from the helpful markers that I discarded the first time round</p>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/2023-08-29/wc-verb-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/2023-08-29/wc-verb-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/2023-08-29/wc-verb-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/2023-08-29/wc-verb.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture>

</figure>

    </div>
</div>
<div class="caption">
    Word cloud, taking only the words marked with VERB marker.
</div>

<p>At this point we can probably get something a little more concrete by counting the number of verbs and how many times each word appears. We ended up with 1671 unique verbs, the top ten are</p>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/2023-08-29/hist-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/2023-08-29/hist-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/2023-08-29/hist-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/2023-08-29/hist.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture>

</figure>

    </div>
</div>

<p>As we have noted before the walking action is quite dominant. Verbs with low count included quite a lot of misspelt words as well as possibly mistagged words such as ‘rotatibg’, ‘leftwhilst’, etc.</p>]]></content><author><name></name></author><category term="random" /><category term="motion" /><summary type="html"><![CDATA[I was curious about the actions in HumanML3D, so I made a simple word cloud out of the descriptions. After removing a few nuisance words (in, at, out, up, to, etc - there were a lot of person and man as well)]]></summary></entry><entry><title type="html">Connection between Lorentz group and rotation group</title><link href="https://hlinhn.github.io/blog/2023/lorentz/" rel="alternate" type="text/html" title="Connection between Lorentz group and rotation group" /><published>2023-06-08T00:00:00+00:00</published><updated>2023-06-08T00:00:00+00:00</updated><id>https://hlinhn.github.io/blog/2023/lorentz</id><content type="html" xml:base="https://hlinhn.github.io/blog/2023/lorentz/"><![CDATA[<p>The rotation group can be thought of as \(3\times 3\) matrices \(R\) that satisfies \(I = R^{T}IR\), while the Lorentz group is \(\eta = \Lambda^{T}\eta\Lambda\), where \(I\) is the Euclidean metric and \(\eta\) is Lorentzian<sup><a href="#1">[1]</a></sup>.</p>

<p><a id="1"><sub>[1]</sub></a>
<sub>Carroll Sean (2003). Spacetime and Geometry - an Introduction to General Relativity</sub></p>]]></content><author><name></name></author><category term="til" /><summary type="html"><![CDATA[The rotation group can be thought of as \(3\times 3\) matrices \(R\) that satisfies \(I = R^{T}IR\), while the Lorentz group is \(\eta = \Lambda^{T}\eta\Lambda\), where \(I\) is the Euclidean metric and \(\eta\) is Lorentzian[1].]]></summary></entry></feed>